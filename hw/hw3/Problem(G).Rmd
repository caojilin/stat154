---
title: "problem(g)"
author: "caojilin"
date: "3/4/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(mclust)
library(MASS)

```

```{r,echo=FALSE}
Z = rbinom(1000 ,size = 1,prob = 0.5) + 1
X = matrix(rep(0, 2000), ncol=2)
mu1  = c(0,0)
sigma1 = diag(2)
mu2  = c(1,0)
sigma2 = matrix(c(1,0,0,2) ,2,2, byrow = T)

for (i in 1:1000) {
  if (Z[i] == 1) {
    obsev = mvrnorm(1, mu1, sigma1)
    X[i,1] = obsev[1]
    X[i,2] = obsev[2]
  }else if(Z[i] == 2){
    obsev = mvrnorm(1, mu2, sigma2)
    X[i,1] = obsev[1]
    X[i,2] = obsev[2]
  }
}
plot(X, col = Z+1, main = "Original")
points(x = 0, y =0 , col = "blue", bg="blue", pch=21)
points(x = 1, y =0 , col = "blue", bg="blue", pch=21)

```

```{r,echo=F}
output1 = kmeans(X, centers = 2)
centers = output1$centers

plot(X, col = output1$cluster+1, main="K-mean")
points(centers, col = "blue", bg="blue", pch=21)
```


```{r,echo=0}
output2 = Mclust(X, G = 2)
print("EM centers")
output2$parameters$mean
plot(X, col = output2$classification+1, main = "EM")
points(output2$parameters$mean, col = "blue", bg="blue", pch=21)

```

K-means only calculates Euclidean distance, which is like a non-parametric approach, while EM assumes there exist underlying distributions and uses likelihood to calculate, which is more like a parametric approach.
We know that original data is almost non-separable. K-mean gives us a "hard"" classification while EM gives us a "soft" classification. Thus the estimated labels are different.